{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxdkBYKSnFTZoQxm9Sou/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lostgreen/FeatureAnalysis/blob/main/StrangersSay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWoq9N-UDDeJ",
        "outputId": "1d16f228-5a1a-4374-aa3d-c00159fde6c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting openai==1.37.1\n",
            "  Downloading openai-1.37.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.37.1) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.37.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.37.1) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.37.1) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.37.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.37.1) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.37.1) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.37.1) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.37.1) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.37.1) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.37.1) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.37.1) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.37.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.37.1) (2.27.1)\n",
            "Downloading openai-1.37.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed openai-1.37.1\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python deepface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNd9Ghf2bRkq",
        "outputId": "acbb3bee-c2b8-4093-971d-52c0463aee4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.66.6)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (11.0.0)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.17.1)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (3.5.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from deepface) (3.0.3)\n",
            "Collecting flask-cors>=4.0.1 (from deepface)\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (2.5.0)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (3.16.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2024.8.30)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=14727f77f23df2b40cd49650fa312eb3c1b0523e2e710e12ca408d82ff10a6f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built fire\n",
            "Installing collected packages: lz4, gunicorn, fire, mtcnn, flask-cors, retina-face, deepface\n",
            "Successfully installed deepface-0.0.93 fire-0.7.0 flask-cors-5.0.0 gunicorn-23.0.0 lz4-4.3.3 mtcnn-1.0.0 retina-face-0.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.56.1\n",
        "!pip install httpx==0.27.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkJJCfMui8J9",
        "outputId": "43ba86e5-1453-45c3-ac03-95a922321587"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.56.1\n",
            "  Downloading openai-1.56.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.56.1) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.56.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.56.1) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.56.1) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.56.1) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.56.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.56.1) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai==1.56.1) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.56.1) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.56.1) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.56.1) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.56.1) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.56.1) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.56.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.56.1) (2.27.1)\n",
            "Downloading openai-1.56.1-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed openai-1.56.1\n",
            "Collecting httpx==0.27.2\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx==0.27.2) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx==0.27.2) (1.2.2)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: httpx\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "Successfully installed httpx-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test deepseek prompt\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
        "    api_key=\"sk-51d2985a4cd14eba9709a9bf31ad0930\",\n",
        "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        ")\n",
        "standards = [\"相关性标准\", \"可接受性标准\", \"充分性标准\"]\n",
        "standard_details = [\"如果一个论点的所有前提都支持该主张的真实性(或虚假性)，则该论点满足相关性标准\", \"如果一个论点的前提代表了无可争议的常识或事实，那么它就符合可接受性标准\", \"如果一个论证的前提提供了足够的证据来接受或拒绝该主张，则该论点就符合充分性标准\"]\n",
        "for standard, standard_detail in zip(standards, standard_details):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"qwen-long\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": f\"你是一个辩论手，需要根据用户提供的辩论稿，识别论据和论点，并对论据进行{standard}打分，仅仅回复0-10之间的一个数字, {standard_detail}\"},\n",
        "        {\"role\": \"user\", \"content\": \"大家好我是子寅,我的观点是要大方走出来,不走出来我在等什么,等他们走吗,敌不动我不动,他们不走,我就一直等着吗,现在厕所坑位多,紧俏,我们写字楼里每一层有400多个员工,每个性别只有两个坑位,早晚高峰时候排队等位那是家常便饭,我在里边,我知道外面有人等我还不出来,这叫什么,占着茅坑不拉屎,哈哈哈所以我要走出来,因为我不想过度占用公共资源,哈哈哈哎,这个这个很妙,哎这个很妙,有人说了,你走出来会被人发现,大家看看我,我这个样子扔到写字楼里,谁能发现我,哈哈哈三个程序员穿着格子衫,往那一站,开心消消乐,哈哈哈往出一走,移动的马赛克,你还发现我,我自己都发现不了我自己,我一扭头,我就以为是在照镜子,哈哈哈哈,哈哈哈哈哈,可以杨幂给漂亮,当然,我跟一般的程序员确实不太一样,因为我太喜欢说话了,但是平时我没有太多机会说话,因为其他程序员不喜欢说话,所以憋的我都来奇葩说了,现在隔间里外面有人议论我,多好的说话机会,我管他好话还是坏话,我先出去痛快痛快嘴,我走过去,我跟他说,哥们聊着呢,哈哈哈这么能说,刚才在工位怎么不说呢,跑厕所说这么热闹,好这口是吧,空气新鲜的地张不开嘴是不是,哈哈哈那咱们就好好说说,不是喜欢串闲话吗,不是喜欢逼逼吗,巧了我奇葩说辩手要的就是i can IBB,咱们今天就看看谁,能BB咱们就看到who is BB,king of tow e lay哈哈哈,有人还说了,我在厕所里不出来是为了不想撕破脸,做人留一线,日后好相见,我告诉你,已经不可能你们回不去了,因为之后你们的关系变成什么样,说你坏话这件事,你知道他知道,他不知道你知道他知道,你又得假装你不知道,他知道陪着他演戏,他对你视而不见,天天留你一个人搁那即兴表演,他没有为说坏话这件事情,付出任何代价,而你徒增了很多烦恼,导致了伤增,伤增是什么,就是徒增了很多烦恼,哈哈哈,所以怎么办,大方走出来呀,在里面听没听别人说坏话,我多难受,凭什么我要自己难受,我就要走出来,让他也难受难受,他看见我的时候,你是不是得看看我,你是不是得琢磨,你是不是得琢磨他听没听到,你是不是得琢磨他第一句话要说什么,哎我还就不说了,哈哈哈我就让你猜,我要让你们猜的茶饭不思,夜不能寐,哎我还不着急走,我还要在里边洗手,我还要记洗手液,我还要认真揉搓30秒,我搓的你灵魂片片凋落哦哦哦,抢我的歌,所以我要大方走出来,我要把我的难受强加给那个始作俑者,其实在职场里走出来,这是一个我们小小的倔强,我就是要走出来,我管你外面有谁,我管你说什么,你说好话我要走出来,你说坏话我还是要走出来,因为我上厕所上门了就得走出来,这是我的开门自由,也是我自由的底线,谁也不能阻,拦哪怕他是王祖蓝,哈哈哈,所以我的观点是要大方走出来,谢谢大\"},\n",
        "    ],\n",
        "    stream=False\n",
        "  )\n",
        "\n",
        "  print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyQZPrO1vbdR",
        "outputId": "a2413411-cbe2-4a7e-ad5d-122d962520c9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "8\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# client = OpenAI(api_key=\"sk-a718985d8ce44bfe91a1a2878c6b35b3\", base_url=\"https://api.deepseek.com\")\n",
        "client = OpenAI(\n",
        "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
        "    api_key=\"sk-51d2985a4cd14eba9709a9bf31ad0930\",\n",
        "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        ")\n",
        "standards = [\"Claim\", \"Premise\", \"Evidence\"]\n",
        "standard_details = [\"辩手在一段发言中的核心论点，表明立场并为自己的发言做一个整体上的总结\",\n",
        "            \"在论证中，前提可以是事实、假设、定义或任何被认为是真实并用来支持进一步推理的陈述\",\n",
        "            \"证据片段是用于支持或证明某个主张的具体事实、数据、引用、统计数字、案例研究或其他形式的实证材料\"]\n",
        "\n",
        "prompt= \"你是一个辩论手，需要把用户提供的辩论稿，根据以下定义，Claim（立论）：辩手在一段发言中的核心论点，表明立场并为自己的发言做一个整体上的总结；Premise（陈述）：辩手对自己提出的论点做补充说明，辩手在表明自己立场之后，会用一段陈述来解释。它们是推理过程中的起点，提供了得出特定结论的基础。在论证中，前提可以是事实、假设、定义或任何被认为是真实并用来支持进一步推理的陈述；Pieces of Evidence（证据片段）: 证据片段是用于支持或证明某个主张的具体事实、数据、引用、统计数字、案例研究或其他形式的实证材料。这些证据片段在论证中充当支持性角色，为前提提供了可观察或可验证的依据，增强了论证的可信度。将辩论稿按照以上三中单元进行划分，注意不要修改原文,不要加入自己的总结。\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"qwen-long\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": \"大家好我是子寅,我的观点是要大方走出来,不走出来我在等什么,等他们走吗,敌不动我不动,他们不走,我就一直等着吗,现在厕所坑位多,紧俏,我们写字楼里每一层有400多个员工,每个性别只有两个坑位,早晚高峰时候排队等位那是家常便饭,我在里边,我知道外面有人等我还不出来,这叫什么,占着茅坑不拉屎,哈哈哈所以我要走出来,因为我不想过度占用公共资源,哈哈哈哎,这个这个很妙,哎这个很妙,有人说了,你走出来会被人发现,大家看看我,我这个样子扔到写字楼里,谁能发现我,哈哈哈三个程序员穿着格子衫,往那一站,开心消消乐,哈哈哈往出一走,移动的马赛克,你还发现我,我自己都发现不了我自己,我一扭头,我就以为是在照镜子,哈哈哈哈,哈哈哈哈哈,可以杨幂给漂亮,当然,我跟一般的程序员确实不太一样,因为我太喜欢说话了,但是平时我没有太多机会说话,因为其他程序员不喜欢说话,所以憋的我都来奇葩说了,现在隔间里外面有人议论我,多好的说话机会,我管他好话还是坏话,我先出去痛快痛快嘴,我走过去,我跟他说,哥们聊着呢,哈哈哈这么能说,刚才在工位怎么不说呢,跑厕所说这么热闹,好这口是吧,空气新鲜的地张不开嘴是不是,哈哈哈那咱们就好好说说,不是喜欢串闲话吗,不是喜欢逼逼吗,巧了我奇葩说辩手要的就是i can IBB,咱们今天就看看谁,能BB咱们就看到who is BB,king of tow e lay哈哈哈,有人还说了,我在厕所里不出来是为了不想撕破脸,做人留一线,日后好相见,我告诉你,已经不可能你们回不去了,因为之后你们的关系变成什么样,说你坏话这件事,你知道他知道,他不知道你知道他知道,你又得假装你不知道,他知道陪着他演戏,他对你视而不见,天天留你一个人搁那即兴表演,他没有为说坏话这件事情,付出任何代价,而你徒增了很多烦恼,导致了伤增,伤增是什么,就是徒增了很多烦恼,哈哈哈,所以怎么办,大方走出来呀,在里面听没听别人说坏话,我多难受,凭什么我要自己难受,我就要走出来,让他也难受难受,他看见我的时候,你是不是得看看我,你是不是得琢磨,你是不是得琢磨他听没听到,你是不是得琢磨他第一句话要说什么,哎我还就不说了,哈哈哈我就让你猜,我要让你们猜的茶饭不思,夜不能寐,哎我还不着急走,我还要在里边洗手,我还要记洗手液,我还要认真揉搓30秒,我搓的你灵魂片片凋落哦哦哦,抢我的歌,所以我要大方走出来,我要把我的难受强加给那个始作俑者,其实在职场里走出来,这是一个我们小小的倔强,我就是要走出来,我管你外面有谁,我管你说什么,你说好话我要走出来,你说坏话我还是要走出来,因为我上厕所上门了就得走出来,这是我的开门自由,也是我自由的底线,谁也不能阻,拦哪怕他是王祖蓝,哈哈哈,所以我的观点是要大方走出来,谢谢大\"},\n",
        "    ],\n",
        "    stream=False\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7TZLv64cRzU",
        "outputId": "7e2dc428-7445-4436-e0fb-9426d2b5d6a8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Claim（立论）\n",
            "- **我的观点是要大方走出来**。\n",
            "\n",
            "### Premise（陈述）\n",
            "1. **等他们走吗，敌不动我不动，他们不走，我就一直等着吗**：\n",
            "   - 现在厕所坑位多，紧俏，我们写字楼里每一层有400多个员工，每个性别只有两个坑位，早晚高峰时候排队等位那是家常便饭。\n",
            "   - 我在里边，我知道外面有人等我还不出来，这叫什么，占着茅坑不拉屎。\n",
            "\n",
            "2. **我走出来，因为我不想过度占用公共资源**：\n",
            "   - 哈哈哈哈，哎，这个这个很妙，哎这个很妙。\n",
            "\n",
            "3. **有人说了，你走出来会被人发现**：\n",
            "   - 大家看看我，我这个样子扔到写字楼里，谁能发现我。\n",
            "   - 三个程序员穿着格子衫，往那一站，开心消消乐。\n",
            "   - 往出一走，移动的马赛克，你还发现我，我自己都发现不了我自己，我一扭头，我就以为是在照镜子，哈哈哈哈，哈哈哈哈哈。\n",
            "\n",
            "4. **我跟一般的程序员确实不太一样，因为我太喜欢说话了**：\n",
            "   - 但是平时我没有太多机会说话，因为其他程序员不喜欢说话，所以憋的我都来奇葩说了。\n",
            "   - 现在隔间里外面有人议论我，多好的说话机会，我管他好话还是坏话，我先出去痛快痛快嘴。\n",
            "   - 我走过去，我跟他说，哥们聊着呢，哈哈哈这么能说，刚才在工位怎么不说呢，跑厕所说这么热闹，好这口是吧，空气新鲜的地张不开嘴是不是，哈哈哈那咱们就好好说说，不是喜欢串闲话吗，不是喜欢逼逼吗，巧了我奇葩说辩手要的就是i can IBB，咱们今天就看看谁，能BB咱们就看到who is BB,king of tow e lay哈哈哈。\n",
            "\n",
            "5. **有人还说了，我在厕所里不出来是为了不想撕破脸，做人留一线，日后好相见**：\n",
            "   - 我告诉你，已经不可能你们回不去了，因为之后你们的关系变成什么样，说你坏话这件事，你知道他知道，他不知道你知道他知道，你又得假装你不知道，他知道陪着他演戏，他对你视而不见，天天留你一个人搁那即兴表演。\n",
            "   - 他没有为说坏话这件事情，付出任何代价，而你徒增了很多烦恼，导致了伤增，伤增是什么，就是徒增了很多烦恼，哈哈哈。\n",
            "\n",
            "6. **在里面听没听别人说坏话，我多难受，凭什么我要自己难受，我就要走出来，让他也难受难受**：\n",
            "   - 他看见我的时候，你是不是得看看我，你是不是得琢磨，你是不是得琢磨他听没听到，你是不是得琢磨他第一句话要说什么，哎我还就不说了，哈哈哈我就让你猜，我要让你们猜的茶饭不思，夜不能寐。\n",
            "   - 我还不着急走，我还要在里边洗手，我还要记洗手液，我还要认真揉搓30秒，我搓的你灵魂片片凋落哦哦哦，抢我的歌。\n",
            "\n",
            "7. **在职场里走出来，这是一个我们小小的倔强**：\n",
            "   - 我就是要走出来，我管你外面有谁，我管你说什么，你说好话我要走出来，你说坏话我还是要走出来，因为我上厕所上门了就得走出来，这是我的开门自由，也是我自由的底线，谁也不能阻拦，哪怕他是王祖蓝，哈哈哈。\n",
            "\n",
            "### Pieces of Evidence（证据片段）\n",
            "- **写字楼里每一层有400多个员工，每个性别只有两个坑位，早晚高峰时候排队等位那是家常便饭**。\n",
            "- **三个程序员穿着格子衫，往那一站，开心消消乐**。\n",
            "- **我在里边，我知道外面有人等我还不出来，这叫什么，占着茅坑不拉屎**。\n",
            "- **我还要记洗手液，我还要认真揉搓30秒**。\n",
            "- **我上厕所上门了就得走出来，这是我的开门自由，也是我自由的底线**。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_adu(text):\n",
        "  client = OpenAI(\n",
        "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
        "    api_key=\"sk-51d2985a4cd14eba9709a9bf31ad0930\",\n",
        "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        "  )\n",
        "\n",
        "  prompt= \"你是一个辩论手，需要把用户提供的辩论稿，根据以下定义，Claim（立论）：辩手在一段发言中的核心论点，表明立场并为自己的发言做一个整体上的总结；Premise（陈述）：辩手对自己提出的论点做补充说明，辩手在表明自己立场之后，会用一段陈述来解释。它们是推理过程中的起点，提供了得出特定结论的基础。在论证中，前提可以是事实、假设、定义或任何被认为是真实并用来支持进一步推理的陈述；Pieces of Evidence（证据片段）: 证据片段是用于支持或证明某个主张的具体事实、数据、引用、统计数字、案例研究或其他形式的实证材料。这些证据片段在论证中充当支持性角色，为前提提供了可观察或可验证的依据，增强了论证的可信度。将辩论稿按照以上三中单元进行划分，注意不要修改原文,不要加入自己的总结。\"\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"qwen-long\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": prompt},\n",
        "          {\"role\": \"user\", \"content\": text},\n",
        "      ],\n",
        "      stream=False\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "CYAC6W7OMCvK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quality_mark(text):\n",
        "  client = OpenAI(api_key=\"sk-51d2985a4cd14eba9709a9bf31ad0930\", base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n",
        "  standards = [\"相关性标准\", \"可接受性标准\", \"充分性标准\"]\n",
        "  judge_result = []\n",
        "  standard_details = [\"如果一个论点的所有前提都支持该主张的真实性(或虚假性)，则该论点满足相关性标准\", \"如果一个论点的前提代表了无可争议的常识或事实，那么它就符合可接受性标准\", \"如果一个论证的前提提供了足够的证据来接受或拒绝该主张，则该论点就符合充分性标准\"]\n",
        "  for standard, standard_detail in zip(standards, standard_details):\n",
        "    response = client.chat.completions.create(\n",
        "      model=\"qwen-long\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": f\"你是一个辩论手，需要根据用户提供的辩论稿，对论点论据进行{standard}打分，综合所有论点论据的分数，形成整个文段的打分，只需要回复0-10的数字, {standard_detail}\"},\n",
        "          {\"role\": \"user\", \"content\": text},\n",
        "      ],\n",
        "      stream=False\n",
        "    )\n",
        "    # print(standard, response.choices[0].message.content)\n",
        "    judge_result.append((standard, response.choices[0].message.content))\n",
        "  return judge_result"
      ],
      "metadata": {
        "id": "WD2MlJJlpIJq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 社会临场感\n",
        "details = [\"情感：从文字的情感，幽默性，情感流露等方面考察\", \"交流性：从提供建议，提问，自我表达等方面考察\", \"凝聚力：从称呼、社交性话语使用等方面考察\"]\n",
        "response = client.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": f\"你是一个辩论手，需要根据用户提供的辩论稿,进行社会临场感打分，综合情感，交流性，凝聚力三种标准，仅仅回复0-10之间的一个数字, {details}\"},\n",
        "        {\"role\": \"user\", \"content\": \"大家好我是子寅,我的观点是要大方走出来,不走出来我在等什么,等他们走吗,敌不动我不动,他们不走,我就一直等着吗,现在厕所坑位多,紧俏,我们写字楼里每一层有400多个员工,每个性别只有两个坑位,早晚高峰时候排队等位那是家常便饭,我在里边,我知道外面有人等我还不出来,这叫什么,占着茅坑不拉屎,哈哈哈所以我要走出来,因为我不想过度占用公共资源,哈哈哈哎,这个这个很妙,哎这个很妙,有人说了,你走出来会被人发现,大家看看我,我这个样子扔到写字楼里,谁能发现我,哈哈哈三个程序员穿着格子衫,往那一站,开心消消乐,哈哈哈往出一走,移动的马赛克,你还发现我,我自己都发现不了我自己,我一扭头,我就以为是在照镜子,哈哈哈哈,哈哈哈哈哈,可以杨幂给漂亮,当然,我跟一般的程序员确实不太一样,因为我太喜欢说话了,但是平时我没有太多机会说话,因为其他程序员不喜欢说话,所以憋的我都来奇葩说了,现在隔间里外面有人议论我,多好的说话机会,我管他好话还是坏话,我先出去痛快痛快嘴,我走过去,我跟他说,哥们聊着呢,哈哈哈这么能说,刚才在工位怎么不说呢,跑厕所说这么热闹,好这口是吧,空气新鲜的地张不开嘴是不是,哈哈哈那咱们就好好说说,不是喜欢串闲话吗,不是喜欢逼逼吗,巧了我奇葩说辩手要的就是i can IBB,咱们今天就看看谁,能BB咱们就看到who is BB,king of tow e lay哈哈哈,有人还说了,我在厕所里不出来是为了不想撕破脸,做人留一线,日后好相见,我告诉你,已经不可能你们回不去了,因为之后你们的关系变成什么样,说你坏话这件事,你知道他知道,他不知道你知道他知道,你又得假装你不知道,他知道陪着他演戏,他对你视而不见,天天留你一个人搁那即兴表演,他没有为说坏话这件事情,付出任何代价,而你徒增了很多烦恼,导致了伤增,伤增是什么,就是徒增了很多烦恼,哈哈哈,所以怎么办,大方走出来呀,在里面听没听别人说坏话,我多难受,凭什么我要自己难受,我就要走出来,让他也难受难受,他看见我的时候,你是不是得看看我,你是不是得琢磨,你是不是得琢磨他听没听到,你是不是得琢磨他第一句话要说什么,哎我还就不说了,哈哈哈我就让你猜,我要让你们猜的茶饭不思,夜不能寐,哎我还不着急走,我还要在里边洗手,我还要记洗手液,我还要认真揉搓30秒,我搓的你灵魂片片凋落哦哦哦,抢我的歌,所以我要大方走出来,我要把我的难受强加给那个始作俑者,其实在职场里走出来,这是一个我们小小的倔强,我就是要走出来,我管你外面有谁,我管你说什么,你说好话我要走出来,你说坏话我还是要走出来,因为我上厕所上门了就得走出来,这是我的开门自由,也是我自由的底线,谁也不能阻,拦哪怕他是王祖蓝,哈哈哈,所以我的观点是要大方走出来,谢谢大\"},\n",
        "    ],\n",
        "    stream=False\n",
        "  )\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1HHCHs4-Iuz",
        "outputId": "14b73b17-42c9-4b9f-cfbb-1360439cd5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_interaction_mark(text):\n",
        "  client = OpenAI(api_key=\"sk-a718985d8ce44bfe91a1a2878c6b35b3\", base_url=\"https://api.deepseek.com\")\n",
        "  details = [\"情感：从文字的情感，幽默性，情感流露等方面考察\", \"交流性：从提供建议，提问，自我表达等方面考察\", \"凝聚力：从称呼、社交性话语使用等方面考察\"]\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"deepseek-chat\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": f\"你是一个辩论手，需要根据用户提供的辩论稿,进行社会临场感打分，综合情感，交流性，凝聚力三种标准，仅仅回复0-10之间的一个数字, {details}\"},\n",
        "          {\"role\": \"user\", \"content\": \"大家好我是子寅,我的观点是要大方走出来,不走出来我在等什么,等他们走吗,敌不动我不动,他们不走,我就一直等着吗,现在厕所坑位多,紧俏,我们写字楼里每一层有400多个员工,每个性别只有两个坑位,早晚高峰时候排队等位那是家常便饭,我在里边,我知道外面有人等我还不出来,这叫什么,占着茅坑不拉屎,哈哈哈所以我要走出来,因为我不想过度占用公共资源,哈哈哈哎,这个这个很妙,哎这个很妙,有人说了,你走出来会被人发现,大家看看我,我这个样子扔到写字楼里,谁能发现我,哈哈哈三个程序员穿着格子衫,往那一站,开心消消乐,哈哈哈往出一走,移动的马赛克,你还发现我,我自己都发现不了我自己,我一扭头,我就以为是在照镜子,哈哈哈哈,哈哈哈哈哈,可以杨幂给漂亮,当然,我跟一般的程序员确实不太一样,因为我太喜欢说话了,但是平时我没有太多机会说话,因为其他程序员不喜欢说话,所以憋的我都来奇葩说了,现在隔间里外面有人议论我,多好的说话机会,我管他好话还是坏话,我先出去痛快痛快嘴,我走过去,我跟他说,哥们聊着呢,哈哈哈这么能说,刚才在工位怎么不说呢,跑厕所说这么热闹,好这口是吧,空气新鲜的地张不开嘴是不是,哈哈哈那咱们就好好说说,不是喜欢串闲话吗,不是喜欢逼逼吗,巧了我奇葩说辩手要的就是i can IBB,咱们今天就看看谁,能BB咱们就看到who is BB,king of tow e lay哈哈哈,有人还说了,我在厕所里不出来是为了不想撕破脸,做人留一线,日后好相见,我告诉你,已经不可能你们回不去了,因为之后你们的关系变成什么样,说你坏话这件事,你知道他知道,他不知道你知道他知道,你又得假装你不知道,他知道陪着他演戏,他对你视而不见,天天留你一个人搁那即兴表演,他没有为说坏话这件事情,付出任何代价,而你徒增了很多烦恼,导致了伤增,伤增是什么,就是徒增了很多烦恼,哈哈哈,所以怎么办,大方走出来呀,在里面听没听别人说坏话,我多难受,凭什么我要自己难受,我就要走出来,让他也难受难受,他看见我的时候,你是不是得看看我,你是不是得琢磨,你是不是得琢磨他听没听到,你是不是得琢磨他第一句话要说什么,哎我还就不说了,哈哈哈我就让你猜,我要让你们猜的茶饭不思,夜不能寐,哎我还不着急走,我还要在里边洗手,我还要记洗手液,我还要认真揉搓30秒,我搓的你灵魂片片凋落哦哦哦,抢我的歌,所以我要大方走出来,我要把我的难受强加给那个始作俑者,其实在职场里走出来,这是一个我们小小的倔强,我就是要走出来,我管你外面有谁,我管你说什么,你说好话我要走出来,你说坏话我还是要走出来,因为我上厕所上门了就得走出来,这是我的开门自由,也是我自由的底线,谁也不能阻,拦哪怕他是王祖蓝,哈哈哈,所以我的观点是要大方走出来,谢谢大\"},\n",
        "      ],\n",
        "      stream=False\n",
        "    )\n",
        "\n",
        "  # print(response.choices[0].message.content)\n",
        "  return [(\"社会临场感\", response.choices[0].message.content)]"
      ],
      "metadata": {
        "id": "fPk6Y75F97fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('1-1_1_小黑.txt', 'r') as file:\n",
        "  debate_speech = file.read()\n",
        "debate_speech = debate_speech.replace('\\n', '')\n",
        "get_quality_mark(debate_speech)\n",
        "get_interaction_mark(debate_speech)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koyoDONbrRqC",
        "outputId": "3effd696-eac6-444b-8e96-261972f2885f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('社会临场感', '8')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "\n",
        "# 使用 pydub 将 mp3 文件转换为 wav 格式\n",
        "def convert_mp3_to_wav(mp3_path):\n",
        "    sound = AudioSegment.from_mp3(mp3_path)\n",
        "    wav_path = mp3_path.replace('.mp3', '.wav')\n",
        "    sound.export(wav_path, format=\"wav\")\n",
        "    return wav_path\n",
        "\n",
        "# 加载音频文件\n",
        "mp3_file = '1-1_2_子寅.mp3'\n",
        "wav_file = convert_mp3_to_wav(mp3_file)\n",
        "\n",
        "# 使用 librosa 加载 wav 文件\n",
        "y, sr = librosa.load(wav_file)\n",
        "\n",
        "# 定义帧长和帧移\n",
        "frame_length = 2048  # 例如 2048 个样本点\n",
        "hop_length = 512     # 例如 512 个样本点\n",
        "\n",
        "# 计算每个帧的 RMS 值\n",
        "frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n",
        "rms_values = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "# 最大最小归一化\n",
        "min_rms = np.min(rms_values)\n",
        "max_rms = np.max(rms_values)\n",
        "normalized_rms = (rms_values - min_rms) / (max_rms - min_rms)\n",
        "\n",
        "# 将归一化后的 RMS 值乘以 100\n",
        "scaled_rms = normalized_rms * 100\n",
        "\n",
        "# 计算所有帧的平均值\n",
        "average_volume = np.mean(scaled_rms)\n",
        "\n",
        "print(f'Average Volume Feature: {average_volume:.2f}')\n",
        "\n",
        "# 清理临时文件\n",
        "import os\n",
        "os.remove(wav_file)"
      ],
      "metadata": {
        "id": "mk5H1xigzSR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da92085-440a-4f9e-b884-5afaf0143dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Volume Feature: 37.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_volume_mark(file_name):\n",
        "  def convert_mp3_to_wav(mp3_path):\n",
        "    sound = AudioSegment.from_mp3(mp3_path)\n",
        "    wav_path = mp3_path.replace('.mp3', '.wav')\n",
        "    sound.export(wav_path, format=\"wav\")\n",
        "    return wav_path\n",
        "\n",
        "  # 加载音频文件\n",
        "  mp3_file = file_name\n",
        "  wav_file = convert_mp3_to_wav(mp3_file)\n",
        "\n",
        "  # 使用 librosa 加载 wav 文件\n",
        "  y, sr = librosa.load(wav_file)\n",
        "\n",
        "  # 定义帧长和帧移\n",
        "  frame_length = 2048  # 例如 2048 个样本点\n",
        "  hop_length = 512     # 例如 512 个样本点\n",
        "\n",
        "  # 计算每个帧的 RMS 值\n",
        "  frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n",
        "  rms_values = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "  # 最大最小归一化\n",
        "  min_rms = np.min(rms_values)\n",
        "  max_rms = np.max(rms_values)\n",
        "  normalized_rms = (rms_values - min_rms) / (max_rms - min_rms)\n",
        "\n",
        "  # 将归一化后的 RMS 值乘以 100\n",
        "  scaled_rms = normalized_rms * 100\n",
        "\n",
        "  # 计算所有帧的平均值\n",
        "  average_volume = np.mean(scaled_rms)\n",
        "\n",
        "  # print(f'Average Volume Feature: {average_volume:.2f}')\n",
        "\n",
        "  # 清理临时文件\n",
        "  os.remove(wav_file)\n",
        "\n",
        "  return (\"声音音量\", average_volume)"
      ],
      "metadata": {
        "id": "ExV45HxbEzjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 00:00:07,166 --> 00:00:08,000\n",
        "def parse_timestamp(timestamp):\n",
        "    h, m, s_ms = timestamp.split(':')\n",
        "    s, ms = s_ms.split(',')\n",
        "    return int(h), int(m), int(s), int(ms)\n",
        "\n",
        "def total_milliseconds(h, m, s, ms):\n",
        "    return h * 3600000 + m * 60000 + s * 1000 + ms\n",
        "\n",
        "def calculate_interval(start_timestamp, end_timestamp):\n",
        "    h1, m1, s1, ms1 = parse_timestamp(start_timestamp)\n",
        "    h2, m2, s2, ms2 = parse_timestamp(end_timestamp)\n",
        "\n",
        "    start_ms = total_milliseconds(h1, m1, s1, ms1)\n",
        "    end_ms = total_milliseconds(h2, m2, s2, ms2)\n",
        "\n",
        "    interval_ms = end_ms - start_ms\n",
        "    return interval_ms\n",
        "\n",
        "start_timestamp = \"00:00:07,166\"\n",
        "end_timestamp = \"00:00:08,000\"\n",
        "\n",
        "interval_ms = calculate_interval(start_timestamp, end_timestamp)\n",
        "print(f\"时间间隔为 {interval_ms} 毫秒\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45ALXkSRJJHg",
        "outputId": "1ce1132c-862e-4006-8155-6b7b7ea95257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "时间间隔为 834 毫秒\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "with open('1-1_1_小黑.srt', 'r') as file:\n",
        "  srt_content = file.read()\n",
        "# 计算WPM\n",
        "def calculate_wpm(text, duration_ms):\n",
        "    word_count = len(text.strip())\n",
        "    minutes = duration_ms / 60000.0\n",
        "    wpm = word_count / minutes\n",
        "    return wpm\n",
        "\n",
        "# 解析SRT文件内容\n",
        "entries = srt_content.strip().split('\\n\\n')\n",
        "wpm_list = []\n",
        "\n",
        "for entry in entries:\n",
        "    lines = entry.split('\\n')\n",
        "    if len(lines) >= 3:\n",
        "        timestamps = lines[1]\n",
        "        start_timestamp, end_timestamp = timestamps.split(' --> ')\n",
        "        text = ' '.join(lines[2:])\n",
        "\n",
        "        duration_ms = calculate_interval(start_timestamp, end_timestamp)\n",
        "        wpm = calculate_wpm(text, duration_ms)\n",
        "        wpm_list.append(wpm)\n",
        "avg_speed = sum(wpm_list) / len(wpm_list)\n",
        "print(f\"Average Speed: {avg_speed:.2f} WPM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wymSEEJK64x",
        "outputId": "b4364f45-f582-4162-9cca-8d68535e5ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Speed: 228.44 WPM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_WPM_mark(file_name):\n",
        "  def parse_timestamp(timestamp):\n",
        "    h, m, s_ms = timestamp.split(':')\n",
        "    s, ms = s_ms.split(',')\n",
        "    return int(h), int(m), int(s), int(ms)\n",
        "\n",
        "  def total_milliseconds(h, m, s, ms):\n",
        "    return h * 3600000 + m * 60000 + s * 1000 + ms\n",
        "\n",
        "  def calculate_interval(start_timestamp, end_timestamp):\n",
        "    h1, m1, s1, ms1 = parse_timestamp(start_timestamp)\n",
        "    h2, m2, s2, ms2 = parse_timestamp(end_timestamp)\n",
        "\n",
        "    start_ms = total_milliseconds(h1, m1, s1, ms1)\n",
        "    end_ms = total_milliseconds(h2, m2, s2, ms2)\n",
        "\n",
        "    interval_ms = end_ms - start_ms\n",
        "    return interval_ms\n",
        "\n",
        "  def calculate_wpm(text, duration_ms):\n",
        "    word_count = len(text.strip())\n",
        "    minutes = duration_ms / 60000.0\n",
        "    wpm = word_count / minutes\n",
        "    return wpm\n",
        "\n",
        "  with open(file_name, 'r') as file:\n",
        "    srt_content = file.read()\n",
        "\n",
        "  # 解析SRT文件内容\n",
        "  entries = srt_content.strip().split('\\n\\n')\n",
        "  wpm_list = []\n",
        "\n",
        "  for entry in entries:\n",
        "      lines = entry.split('\\n')\n",
        "      if len(lines) >= 3:\n",
        "          timestamps = lines[1]\n",
        "          start_timestamp, end_timestamp = timestamps.split(' --> ')\n",
        "          text = ' '.join(lines[2:])\n",
        "\n",
        "          duration_ms = calculate_interval(start_timestamp, end_timestamp)\n",
        "          wpm = calculate_wpm(text, duration_ms)\n",
        "          wpm_list.append(wpm)\n",
        "  avg_speed = sum(wpm_list) / len(wpm_list)\n",
        "\n",
        "  return (\"每分钟平均吐字\", int(avg_speed))\n"
      ],
      "metadata": {
        "id": "Sc6xHCZRL5KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoConfig, Wav2Vec2FeatureExtractor, HubertPreTrainedModel, HubertModel\n",
        "\n",
        "model_name_or_path = \"xmj2002/hubert-base-ch-speech-emotion-recognition\"\n",
        "duration = 6\n",
        "sample_rate = 16000\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name_or_path,\n",
        ")\n",
        "\n",
        "\n",
        "def id2class(id):\n",
        "    if id == 0:\n",
        "        return \"angry\"\n",
        "    elif id == 1:\n",
        "        return \"fear\"\n",
        "    elif id == 2:\n",
        "        return \"happy\"\n",
        "    elif id == 3:\n",
        "        return \"neutral\"\n",
        "    elif id == 4:\n",
        "        return \"sad\"\n",
        "    else:\n",
        "        return \"surprise\"\n",
        "\n",
        "\n",
        "def predict(path, processor, model):\n",
        "    speech, sr = librosa.load(path=path, sr=sample_rate)\n",
        "    speech = processor(speech, padding=\"max_length\", truncation=True, max_length=duration * sr,\n",
        "                       return_tensors=\"pt\", sampling_rate=sr).input_values\n",
        "    with torch.no_grad():\n",
        "        logit = model(speech)\n",
        "    score = F.softmax(logit, dim=1).detach().cpu().numpy()[0]\n",
        "    id = torch.argmax(logit).cpu().numpy()\n",
        "    print(f\"file path: {path} \\t predict: {id2class(id)} \\t score:{score[id]} \")\n",
        "\n",
        "\n",
        "class HubertClassificationHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.classifier_dropout)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HubertForSpeechClassification(HubertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.hubert = HubertModel(config)\n",
        "        self.classifier = HubertClassificationHead(config)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.hubert(x)\n",
        "        hidden_states = outputs[0]\n",
        "        x = torch.mean(hidden_states, dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)\n",
        "model = HubertForSpeechClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    config=config,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "def convert_mp3_to_wav(mp3_path):\n",
        "    sound = AudioSegment.from_mp3(mp3_path)\n",
        "    wav_path = mp3_path.replace('.mp3', '.wav')\n",
        "    sound.export(wav_path, format=\"wav\")\n",
        "    return wav_path\n",
        "\n",
        "\n",
        "def parse_srt(srt_path):\n",
        "    with open(srt_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    timestamps = []\n",
        "    current_timestamp = None\n",
        "\n",
        "    for line in lines:\n",
        "        if re.match(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', line):\n",
        "            start, end = line.strip().split(' --> ')\n",
        "            start_time = sum(float(x) * 60 ** i for i, x in enumerate(reversed(start.split(',')[0].split(':'))))\n",
        "            end_time = sum(float(x) * 60 ** i for i, x in enumerate(reversed(end.split(',')[0].split(':'))))\n",
        "            timestamps.append((start_time, end_time))\n",
        "\n",
        "    return timestamps\n",
        "\n",
        "def split_audio_by_timestamps(mp3_path, timestamps, output_dir):\n",
        "    sound = AudioSegment.from_mp3(mp3_path)\n",
        "    for i, (start, end) in enumerate(timestamps):\n",
        "        start_ms = int(start * 1000)\n",
        "        end_ms = int(end * 1000)\n",
        "        segment = sound[start_ms:end_ms]\n",
        "        segment_path = os.path.join(output_dir, f\"segment_{i}.wav\")\n",
        "        segment.export(segment_path, format=\"wav\")\n",
        "        yield segment_path\n",
        "\n",
        "# SRT文件路径\n",
        "srt_file = \"/content/1-1_5_欧阳奕奕.srt\"\n",
        "mp3_file = \"/content/1-1_5_欧阳奕奕.mp3\"\n",
        "\n",
        "output_dir = \"segments\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 解析SRT文件\n",
        "timestamps = parse_srt(srt_file)\n",
        "\n",
        "# 转换并分割音频\n",
        "for segment_path in split_audio_by_timestamps(mp3_file, timestamps, output_dir):\n",
        "    predict(segment_path, processor, model)\n",
        "# 加载音频文件\n",
        "# mp3_file = \"/content/1-1_2_子寅.mp3\"\n",
        "# wav_file = convert_mp3_to_wav(mp3_file)\n",
        "\n",
        "# predict(wav_file, processor, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lvcENIvQvyW",
        "outputId": "0603b515-8468-4dd1-cee2-5ca1b4c04227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file path: segments/segment_0.wav \t predict: angry \t score:0.9993315935134888 \n",
            "file path: segments/segment_1.wav \t predict: angry \t score:0.9997509121894836 \n",
            "file path: segments/segment_2.wav \t predict: happy \t score:0.9983776807785034 \n",
            "file path: segments/segment_3.wav \t predict: happy \t score:0.9632489681243896 \n",
            "file path: segments/segment_4.wav \t predict: angry \t score:0.8026885986328125 \n",
            "file path: segments/segment_5.wav \t predict: happy \t score:0.8099783658981323 \n",
            "file path: segments/segment_6.wav \t predict: angry \t score:0.9423356652259827 \n",
            "file path: segments/segment_7.wav \t predict: angry \t score:0.9691129922866821 \n",
            "file path: segments/segment_8.wav \t predict: surprise \t score:0.9966526627540588 \n",
            "file path: segments/segment_9.wav \t predict: angry \t score:0.9974581599235535 \n",
            "file path: segments/segment_10.wav \t predict: angry \t score:0.9992498755455017 \n",
            "file path: segments/segment_11.wav \t predict: surprise \t score:0.9973958730697632 \n",
            "file path: segments/segment_12.wav \t predict: surprise \t score:0.9998598098754883 \n",
            "file path: segments/segment_13.wav \t predict: happy \t score:0.986947774887085 \n",
            "file path: segments/segment_14.wav \t predict: surprise \t score:0.9999396800994873 \n",
            "file path: segments/segment_15.wav \t predict: angry \t score:0.9983863830566406 \n",
            "file path: segments/segment_16.wav \t predict: surprise \t score:0.9825604557991028 \n",
            "file path: segments/segment_17.wav \t predict: surprise \t score:0.9983431100845337 \n",
            "file path: segments/segment_18.wav \t predict: angry \t score:0.9859693050384521 \n",
            "file path: segments/segment_19.wav \t predict: surprise \t score:0.9754442572593689 \n",
            "file path: segments/segment_20.wav \t predict: surprise \t score:0.9994001388549805 \n",
            "file path: segments/segment_21.wav \t predict: surprise \t score:0.9998974800109863 \n",
            "file path: segments/segment_22.wav \t predict: surprise \t score:0.9999362230300903 \n",
            "file path: segments/segment_23.wav \t predict: surprise \t score:0.9999390840530396 \n",
            "file path: segments/segment_24.wav \t predict: surprise \t score:0.9968979358673096 \n",
            "file path: segments/segment_25.wav \t predict: surprise \t score:0.9998807907104492 \n",
            "file path: segments/segment_26.wav \t predict: surprise \t score:0.9062233567237854 \n",
            "file path: segments/segment_27.wav \t predict: surprise \t score:0.9990334510803223 \n",
            "file path: segments/segment_28.wav \t predict: surprise \t score:0.9997286200523376 \n",
            "file path: segments/segment_29.wav \t predict: surprise \t score:0.998607337474823 \n",
            "file path: segments/segment_30.wav \t predict: angry \t score:0.9344310164451599 \n",
            "file path: segments/segment_31.wav \t predict: surprise \t score:0.9998351335525513 \n",
            "file path: segments/segment_32.wav \t predict: angry \t score:0.9995027780532837 \n",
            "file path: segments/segment_33.wav \t predict: surprise \t score:0.9427412748336792 \n",
            "file path: segments/segment_34.wav \t predict: surprise \t score:0.9999303817749023 \n",
            "file path: segments/segment_35.wav \t predict: surprise \t score:0.9978093504905701 \n",
            "file path: segments/segment_36.wav \t predict: angry \t score:0.9869317412376404 \n",
            "file path: segments/segment_37.wav \t predict: surprise \t score:0.9997438788414001 \n",
            "file path: segments/segment_38.wav \t predict: angry \t score:0.999531626701355 \n",
            "file path: segments/segment_39.wav \t predict: angry \t score:0.999891996383667 \n",
            "file path: segments/segment_40.wav \t predict: angry \t score:0.999729573726654 \n",
            "file path: segments/segment_41.wav \t predict: angry \t score:0.9999566078186035 \n",
            "file path: segments/segment_42.wav \t predict: angry \t score:0.9999537467956543 \n",
            "file path: segments/segment_43.wav \t predict: angry \t score:0.9877877831459045 \n",
            "file path: segments/segment_44.wav \t predict: fear \t score:0.9749255180358887 \n",
            "file path: segments/segment_45.wav \t predict: angry \t score:0.4924013912677765 \n",
            "file path: segments/segment_46.wav \t predict: angry \t score:0.5175315141677856 \n",
            "file path: segments/segment_47.wav \t predict: surprise \t score:0.9874162077903748 \n",
            "file path: segments/segment_48.wav \t predict: surprise \t score:0.9935929179191589 \n",
            "file path: segments/segment_49.wav \t predict: surprise \t score:0.9997349381446838 \n",
            "file path: segments/segment_50.wav \t predict: surprise \t score:0.9019550085067749 \n",
            "file path: segments/segment_51.wav \t predict: surprise \t score:0.9998683929443359 \n",
            "file path: segments/segment_52.wav \t predict: surprise \t score:0.7390282154083252 \n",
            "file path: segments/segment_53.wav \t predict: surprise \t score:0.9990441203117371 \n",
            "file path: segments/segment_54.wav \t predict: surprise \t score:0.9771878123283386 \n",
            "file path: segments/segment_55.wav \t predict: angry \t score:0.9995869994163513 \n",
            "file path: segments/segment_56.wav \t predict: surprise \t score:0.99836665391922 \n",
            "file path: segments/segment_57.wav \t predict: surprise \t score:0.9998602867126465 \n",
            "file path: segments/segment_58.wav \t predict: surprise \t score:0.9997758269309998 \n",
            "file path: segments/segment_59.wav \t predict: angry \t score:0.999404788017273 \n",
            "file path: segments/segment_60.wav \t predict: angry \t score:0.9987357258796692 \n",
            "file path: segments/segment_61.wav \t predict: surprise \t score:0.9992485642433167 \n",
            "file path: segments/segment_62.wav \t predict: surprise \t score:0.998561680316925 \n",
            "file path: segments/segment_63.wav \t predict: surprise \t score:0.9945672750473022 \n",
            "file path: segments/segment_64.wav \t predict: surprise \t score:0.9934105277061462 \n",
            "file path: segments/segment_65.wav \t predict: neutral \t score:0.37910228967666626 \n",
            "file path: segments/segment_66.wav \t predict: surprise \t score:0.9997159838676453 \n",
            "file path: segments/segment_67.wav \t predict: surprise \t score:0.9903002977371216 \n",
            "file path: segments/segment_68.wav \t predict: angry \t score:0.5916535258293152 \n",
            "file path: segments/segment_69.wav \t predict: angry \t score:0.999681830406189 \n",
            "file path: segments/segment_70.wav \t predict: surprise \t score:0.9998434782028198 \n",
            "file path: segments/segment_71.wav \t predict: surprise \t score:0.9178804159164429 \n",
            "file path: segments/segment_72.wav \t predict: surprise \t score:0.9142826199531555 \n",
            "file path: segments/segment_73.wav \t predict: surprise \t score:0.9998694658279419 \n",
            "file path: segments/segment_74.wav \t predict: angry \t score:0.8778852820396423 \n",
            "file path: segments/segment_75.wav \t predict: angry \t score:0.9583254456520081 \n",
            "file path: segments/segment_76.wav \t predict: angry \t score:0.9997342228889465 \n",
            "file path: segments/segment_77.wav \t predict: angry \t score:0.9980729818344116 \n",
            "file path: segments/segment_78.wav \t predict: surprise \t score:0.9869454503059387 \n",
            "file path: segments/segment_79.wav \t predict: angry \t score:0.8255786299705505 \n",
            "file path: segments/segment_80.wav \t predict: angry \t score:0.9993188381195068 \n",
            "file path: segments/segment_81.wav \t predict: surprise \t score:0.9806071519851685 \n",
            "file path: segments/segment_82.wav \t predict: surprise \t score:0.999716579914093 \n",
            "file path: segments/segment_83.wav \t predict: happy \t score:0.7234963774681091 \n",
            "file path: segments/segment_84.wav \t predict: neutral \t score:0.37910228967666626 \n",
            "file path: segments/segment_85.wav \t predict: angry \t score:0.9907038807868958 \n",
            "file path: segments/segment_86.wav \t predict: surprise \t score:0.9993320107460022 \n",
            "file path: segments/segment_87.wav \t predict: angry \t score:0.9976909160614014 \n",
            "file path: segments/segment_88.wav \t predict: surprise \t score:0.9995934367179871 \n",
            "file path: segments/segment_89.wav \t predict: surprise \t score:0.9999133348464966 \n",
            "file path: segments/segment_90.wav \t predict: surprise \t score:0.9998555183410645 \n",
            "file path: segments/segment_91.wav \t predict: surprise \t score:0.9999035596847534 \n",
            "file path: segments/segment_92.wav \t predict: surprise \t score:0.9936034083366394 \n",
            "file path: segments/segment_93.wav \t predict: happy \t score:0.692755401134491 \n",
            "file path: segments/segment_94.wav \t predict: surprise \t score:0.6014130115509033 \n",
            "file path: segments/segment_95.wav \t predict: angry \t score:0.9951468110084534 \n",
            "file path: segments/segment_96.wav \t predict: happy \t score:0.9986066222190857 \n",
            "file path: segments/segment_97.wav \t predict: angry \t score:0.9536528587341309 \n",
            "file path: segments/segment_98.wav \t predict: happy \t score:0.9767783880233765 \n",
            "file path: segments/segment_99.wav \t predict: angry \t score:0.43987879157066345 \n",
            "file path: segments/segment_100.wav \t predict: angry \t score:0.9992433786392212 \n",
            "file path: segments/segment_101.wav \t predict: angry \t score:0.9955493211746216 \n",
            "file path: segments/segment_102.wav \t predict: angry \t score:0.9595040678977966 \n",
            "file path: segments/segment_103.wav \t predict: surprise \t score:0.8279471397399902 \n",
            "file path: segments/segment_104.wav \t predict: angry \t score:0.9125841856002808 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from deepface import DeepFace\n",
        "\n",
        "# 视频文件路径\n",
        "video_path = '1-1_5_欧阳奕奕.mp4'\n",
        "\n",
        "# 打开视频文件\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "def analyze_frame(frame):\n",
        "    # 使用DeepFace进行面部表情分析\n",
        "    try:\n",
        "        result = DeepFace.analyze(img_path=frame, actions=['emotion'], enforce_detection=False)\n",
        "        dominant_emotion = result[0]['dominant_emotion']\n",
        "        print(\"Emotion:\", dominant_emotion)\n",
        "        return dominant_emotion\n",
        "    except ValueError as e:\n",
        "        # 如果没有检测到人脸，则打印错误信息\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "# 读取视频帧并调用分析函数\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        # 如果读取帧失败，说明视频已经结束\n",
        "        print(\"Video has ended.\")\n",
        "        break\n",
        "\n",
        "    # 调用函数分析当前帧\n",
        "    emotion = analyze_frame(frame)\n",
        "\n",
        "    # 如果检测到表情，显示在视频帧上\n",
        "    if emotion:\n",
        "        cv2.putText(frame, f\"Emotion: {emotion}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "    # 延迟一段时间，以便视频帧可以正常显示\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# 释放资源\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "2TnEopv-cRYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# 获取当前工作目录\n",
        "current_directory = \"/content/\"\n",
        "# 特征表\n",
        "feature_dict = defaultdict(list)\n",
        "\n",
        "# 遍历当前目录及其所有子目录\n",
        "for root, dirs, files in os.walk(current_directory):\n",
        "  for name in files:\n",
        "    # 如果是txt文件，则调用deepseek提取论据质量特征\n",
        "    if name.endswith('.txt'):\n",
        "      file_path = os.path.join(root, name)\n",
        "      print(f\"File: {file_path}\")\n",
        "      with open(file_path, 'r') as file:\n",
        "        debate_speech = file.read()\n",
        "      debate_speech = debate_speech.replace('\\n', '')\n",
        "      name = name.split('.')[0]\n",
        "      feature_dict[name].extend(get_quality_mark(debate_speech))\n",
        "      feature_dict[name].extend(get_interaction_mark(debate_speech))\n",
        "    if name.endswith('.mp3'):\n",
        "      file_path = os.path.join(root, name)\n",
        "      print(f\"File: {file_path}\")\n",
        "      feature_dict[name.split('.')[0]].append(get_volume_mark(file_path))\n",
        "    if name.endswith('.srt'):\n",
        "      file_path = os.path.join(root, name)\n",
        "      print(f\"File: {file_path}\")\n",
        "      feature_dict[name.split('.')[0]].append(get_WPM_mark(file_path))\n",
        "\n",
        "feature_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynhR6GBrl40r",
        "outputId": "27986564-4b9e-43b7-b7c1-a8ba84ccd8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: /content/1-1_1_小黑.txt\n",
            "File: /content/1-1_5_欧阳奕奕.mp3\n",
            "File: /content/1-1_2_子寅.mp3\n",
            "File: /content/1-1_2_子寅.srt\n",
            "File: /content/1-1_3_子寅.txt\n",
            "File: /content/1-1_3_子寅.srt\n",
            "File: /content/1-1_1_小黑.mp3\n",
            "File: /content/1-1_4_小黑.txt\n",
            "File: /content/1-1_2_子寅.txt\n",
            "File: /content/1-1_4_小黑.srt\n",
            "File: /content/1-1_3_子寅.mp3\n",
            "File: /content/1-1_1_小黑.srt\n",
            "File: /content/1-1_5_欧阳奕奕.srt\n",
            "File: /content/1-1_4_小黑.mp3\n",
            "File: /content/1-1_5_欧阳奕奕.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'1-1_1_小黑': [('相关性标准', '1'),\n",
              "              ('可接受性标准', '2'),\n",
              "              ('充分性标准', '1'),\n",
              "              ('社会临场感', '8'),\n",
              "              ('声音音量', 28.355072),\n",
              "              ('每分钟平均吐字', 228)],\n",
              "             '1-1_5_欧阳奕奕': [('声音音量', 26.390614),\n",
              "              ('每分钟平均吐字', 312),\n",
              "              ('相关性标准', '7'),\n",
              "              ('可接受性标准', '7'),\n",
              "              ('充分性标准', '6'),\n",
              "              ('社会临场感', '8')],\n",
              "             '1-1_2_子寅': [('声音音量', 37.92442),\n",
              "              ('每分钟平均吐字', 345),\n",
              "              ('相关性标准', '8'),\n",
              "              ('可接受性标准', '8'),\n",
              "              ('充分性标准', '8'),\n",
              "              ('社会临场感', '8')],\n",
              "             '1-1_3_子寅': [('相关性标准', '4'),\n",
              "              ('可接受性标准', '5'),\n",
              "              ('充分性标准', '5'),\n",
              "              ('社会临场感', '8'),\n",
              "              ('每分钟平均吐字', 393),\n",
              "              ('声音音量', 32.27137)],\n",
              "             '1-1_4_小黑': [('相关性标准', '9'),\n",
              "              ('可接受性标准', '8'),\n",
              "              ('充分性标准', '7'),\n",
              "              ('社会临场感', '8'),\n",
              "              ('每分钟平均吐字', 294),\n",
              "              ('声音音量', 36.386124)]})"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ]
}